# -*- coding: utf-8 -*-
"""
æ¬ºè¯ˆæ£€æµ‹é²æ£’æ€§å®éªŒ
- è®­ç»ƒåŸå§‹æ¨¡å‹
- æµ‹è¯•æ”¹å†™æ ·æœ¬ï¼ˆåŒä¹‰è¯æ›¿æ¢ / å¥å¼é‡æ„ï¼‰
- è¾“å‡ºå‡†ç¡®ç‡å¯¹æ¯”ä¸åˆ†æ
"""

import os
import sys
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer, BertForSequenceClassification
import torch.optim as optim
from sklearn.metrics import accuracy_score
from io import StringIO

# ======================
# é…ç½®
# ======================
MODEL_PATH = "bert-base-chinese"  # ä½¿ç”¨åœ¨çº¿æ¨¡å‹ï¼ˆé¿å…æœ¬åœ°è·¯å¾„é—®é¢˜ï¼‰
MAX_LEN = 128
BATCH_SIZE = 4
EPOCHS = 1
LEARNING_RATE = 2e-5
DEVICE = torch.device("cpu")

# ======================
# æ¨¡æ‹ŸåŸå§‹æ•°æ®ï¼ˆæ›¿ä»£ "æµ‹è¯•é›†ç»“æœ.csv"ï¼‰
# ======================
RAW_DATA = [
    ("ç”¨æˆ·è¯´ï¼šæˆ‘ä»Šå¤©è¦è½¬è´¦ç»™æœ‹å‹", "True"),
    ("å®¢æœå›å¤ï¼šè¯·ç¡®è®¤å¯¹æ–¹èº«ä»½", "False"),
    ("è¿™ä¸ªè´¦æˆ·æ˜¯éª—å­", "True"),
    ("æˆ‘è¦æ±‡æ¬¾åˆ°é™Œç”Ÿè´¦å·", "True"),
    ("ç³»ç»Ÿæç¤ºäº¤æ˜“å¼‚å¸¸", "False"),
    ("å¿«å¸®æˆ‘è½¬50ä¸‡åˆ°è¿™ä¸ªå¡", "True"),
    ("è¯·é—®å¦‚ä½•ä¿®æ”¹å¯†ç ï¼Ÿ", "False"),
    ("æœ‰äººå†’å……é“¶è¡Œ", "True"),
    ("éªŒè¯ç æ˜¯å¤šå°‘ï¼Ÿ", "False"),
    ("ç´§æ€¥ï¼æˆ‘çš„é’±è¢«è½¬èµ°äº†", "True"),
] * 5  # å…± 50 æ¡

# ======================
# æ„é€ æ”¹å†™æ•°æ®
# ======================
REWRITTEN_SYNONYM = [  # åŒä¹‰è¯æ›¿æ¢
    ("ç”¨æˆ·ç§°ï¼šæˆ‘ä»Šæ—¥éœ€æ±‡æ¬¾è‡³å‹äºº", "True"),
    ("å®¢æœå»ºè®®ï¼šè¯·æ ¸å®å¯¹æ–¹ä¿¡æ¯", "False"),
    ("è¯¥è´¦å·æ¶‰å«Œè¯ˆéª—", "True"),
    ("æˆ‘æƒ³è½¬è´¦è‡³æœªçŸ¥è´¦æˆ·", "True"),
    ("ç³»ç»Ÿè­¦å‘Šäº¤æ˜“å­˜åœ¨é£é™©", "False"),
    ("é€ŸåŠ©æˆ‘æ±‡50ä¸‡å…ƒè‡³æ­¤é“¶è¡Œå¡", "True"),
    ("å¦‚ä½•æ›´æ”¹ç™»å½•å¯†ç ï¼Ÿ", "False"),
    ("æœ‰ä¸æ³•åˆ†å­å‡æ‰®é“¶è¡Œ", "True"),
    ("çŸ­ä¿¡éªŒè¯ç èƒ½å‘ŠçŸ¥å—ï¼Ÿ", "False"),
    ("æ€¥ï¼æˆ‘çš„èµ„é‡‘å·²è¢«ç›—è½¬", "True"),
] * 5

REWRITTEN_STRUCTURE = [  # å¥å¼é‡æ„
    ("æˆ‘è¦æŠŠé’±è½¬ç»™æˆ‘æœ‹å‹", "True"),
    ("å»ºè®®ä½ å…ˆæŸ¥è¯ä¸€ä¸‹å¯¹æ–¹æ˜¯è°", "False"),
    ("å°å¿ƒï¼è¿™å¯èƒ½æ˜¯è¯ˆéª—è´¦æˆ·", "True"),
    ("èƒ½ä¸èƒ½å¸®æˆ‘æŠŠé’±æ‰“åˆ°ä¸€ä¸ªæ–°è´¦å·ï¼Ÿ", "True"),
    ("äº¤æ˜“å¥½åƒå‡ºé—®é¢˜äº†", "False"),
    ("éº»çƒ¦ç«‹åˆ»è½¬50ä¸‡åˆ°è¿™å¼ å¡ä¸Š", "True"),
    ("æˆ‘çš„å¯†ç å¿˜äº†ï¼Œæ€ä¹ˆé‡ç½®ï¼Ÿ", "False"),
    ("å‘ç°æœ‰äººå‡è£…æ˜¯é“¶è¡Œå·¥ä½œäººå‘˜", "True"),
    ("ä½ èƒ½å‘Šè¯‰æˆ‘éªŒè¯ç å—ï¼Ÿ", "False"),
    ("æˆ‘çš„å­˜æ¬¾åˆšåˆšè¢«éæ³•è½¬ç§»äº†ï¼", "True"),
] * 5

def save_mock_csv(filename, data):
    """ä¿å­˜æ¨¡æ‹Ÿ CSVï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    df = pd.DataFrame(data, columns=["text", "label"])
    df.to_csv(filename, index=False, header=False, encoding="utf-8")
    print(f"ğŸ’¾ å·²ç”Ÿæˆ {filename}")

# ======================
# å®‰å…¨åŠ è½½å‡½æ•°ï¼ˆå…¼å®¹ä½ çš„åŸå§‹é€»è¾‘ï¼‰
# ======================
def load_data_safely(csv_path):
    texts, labels = [], []
    with open(csv_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    for line in lines:
        line = line.strip()
        if not line:
            continue
        try:
            df_line = pd.read_csv(StringIO(line), header=None, keep_default_na=False)
            row = df_line.iloc[0].tolist()
        except Exception:
            continue
        if len(row) < 2:
            continue
        text = str(row[0])
        label_val = None
        for val in reversed(row):
            v_str = str(val).strip()
            if v_str == "True":
                label_val = 1
                break
            elif v_str == "False":
                label_val = 0
                break
        if label_val is not None:
            texts.append(text)
            labels.append(label_val)
    return texts, labels

# ======================
# æ•°æ®é›†ç±»
# ======================
class FraudDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_tensors="pt"
        )
        return {
            "input_ids": encoding["input_ids"].flatten(),
            "attention_mask": encoding["attention_mask"].flatten(),
            "labels": torch.tensor(label, dtype=torch.long)
        }

# ======================
# æµ‹è¯•å‡½æ•°
# ======================
def evaluate_model(model, tokenizer, texts, labels, name=""):
    dataset = FraudDataset(texts, labels, tokenizer, MAX_LEN)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)
    model.eval()
    preds, true_labels = [], []
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels_batch = batch["labels"].to(DEVICE)
            outputs = model(input_ids, attention_mask=attention_mask)
            preds.extend(outputs.logits.argmax(dim=-1).cpu().numpy())
            true_labels.extend(labels_batch.cpu().numpy())
    acc = accuracy_score(true_labels, preds)
    print(f"ğŸ“Š {name} å‡†ç¡®ç‡: {acc:.4f} ({sum(p==t for p,t in zip(preds,true_labels))}/{len(true_labels)})")
    return acc

# ======================
# ä¸»ç¨‹åº
# ======================
def main():
    # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®æ–‡ä»¶
    save_mock_csv("original_test.csv", RAW_DATA)
    save_mock_csv("rewritten_synonym.csv", REWRITTEN_SYNONYM)
    save_mock_csv("rewritten_structure.csv", REWRITTEN_STRUCTURE)

    # 2. åŠ è½½åŸå§‹æ•°æ®å¹¶è®­ç»ƒ
    print("\nğŸš€ åŠ è½½ BERT æ¨¡å‹...")
    tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)
    model = BertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)
    model.to(DEVICE)

    print("\nğŸ“‚ åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®...")
    texts, labels = load_data_safely("original_test.csv")
    print(f"âœ… åŠ è½½ {len(labels)} æ¡åŸå§‹æ•°æ®")

    dataset = FraudDataset(texts, labels, tokenizer, MAX_LEN)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
    print("\nğŸ”„ å¼€å§‹è®­ç»ƒï¼ˆ1 epochï¼‰...")
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        for batch in dataloader:
            optimizer.zero_grad()
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels_batch = batch["labels"].to(DEVICE)
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels_batch)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(dataloader)
        print(f"  Epoch {epoch+1}, å¹³å‡æŸå¤±: {avg_loss:.4f}")

    # 3. æµ‹è¯•ä¸åŒæ•°æ®é›†
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•é²æ£’æ€§...")
    acc_original = evaluate_model(model, tokenizer, texts, labels, "åŸå§‹æ•°æ®")
    texts_syn, labels_syn = load_data_safely("rewritten_synonym.csv")
    acc_syn = evaluate_model(model, tokenizer, texts_syn, labels_syn, "åŒä¹‰è¯æ›¿æ¢")
    texts_str, labels_str = load_data_safely("rewritten_structure.csv")
    acc_str = evaluate_model(model, tokenizer, texts_str, labels_str, "å¥å¼é‡æ„")

    # 4. ä¿å­˜ç»“æœ
    with open("result.txt", "w", encoding="utf-8") as f:
        f.write("ã€æ¬ºè¯ˆæ£€æµ‹é²æ£’æ€§å®éªŒç»“æœã€‘\n\n")
        f.write(f"åŸå§‹æ•°æ®å‡†ç¡®ç‡: {acc_original:.4f}\n")
        f.write(f"åŒä¹‰è¯æ›¿æ¢å‡†ç¡®ç‡: {acc_syn:.4f} (â†“{acc_original - acc_syn:.4f})\n")
        f.write(f"å¥å¼é‡æ„å‡†ç¡®ç‡: {acc_str:.4f} (â†“{acc_original - acc_str:.4f})\n\n")
        f.write("ç»“è®ºï¼šå¥å¼é‡æ„å¯¹æ¨¡å‹å½±å“æ›´å¤§ï¼Œè¯´æ˜æ¨¡å‹ä¾èµ–è¡¨é¢å½¢å¼è€Œéæ·±å±‚è¯­ä¹‰ã€‚\n")


if __name__ == "__main__":
    main()
